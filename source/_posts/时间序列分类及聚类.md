---
title: 时间序列算法的研究
---
# 时间序列算法的研究 
****

在典型的分类问题中，您将获得一组输入要素和一组离散输出类，并且您希望对两者之间的关系进行建模。
你可以使用无数的分类算法来解决这个问题 - 支持向量机，朴素贝叶斯，k-NN等。
但是如果输入特征不是独立的，比如时间序列数据呢？
在这种情况下，SVM和朴素贝叶斯不是一个好的选择，因为他们假设输入特征是独立的。 k-NN算法仍然可以工作，但它依赖于输入示例之间的相似性度量的概念。现在问题变成_我们如何衡量两个时间序列_之间的相似性？

### 假如使用欧式距离方法
两个时间序列 Q 和长度 n 的 C 之间的欧几里德距离定义为

$$d(Q,C) = \sqrt{\sum^n_{i=1}[Q(i)-C(i)]^2}$$

乍一看，似乎只是简单地计算两个时间序列之间的欧几里德距离就可以让我们对它们之间的相似性有一个很好的了解。毕竟，相同时间序列之间的欧几里德距离为零，并且非常不同的时间序列之间的欧几里德距离很大。然而，在我们确定欧几里德距离作为相似性度量之前，我们应该清楚地说明我们所需的标准，以确定两个时间序列之间的相似性

** 然而。。**
*通过良好的相似性度量，两个时间序列中的微小变化应导致其相似性的微小变化。关于欧几里德距离，这对于y轴的变化是正确的，但是对于时间轴的变化（即压缩和拉伸）则不然。请考虑以下示例。*


```python
import pandas as pd
import numpy as np
import matplotlib.pylab as plt
import qgrid


x=np.linspace(0,50,100)
ts1=pd.Series(3.1*np.sin(x/1.5)+3.5)
ts2=pd.Series(2.2*np.sin(x/3.5+2.4)+3.2)
ts3=pd.Series(0.04*x+3.0)
test=pd.DataFrame(x)
test=qgrid.show_grid(test,show_toolbar=True)
test
ts1.plot()
ts2.plot()
ts3.plot()

plt.ylim(-2,10)
plt.legend(['ts1','ts2','ts3'])
plt.show()
test
```


![png](https://thumbnail0.baidupcs.com/thumbnail/75545d55728341603c3285b27578d882?fid=1076969831-250528-304040274284946&time=1537837200&rt=sh&sign=FDTAER-DCb740ccc5511e5e8fedcff06b081203-H4ONS0zHZjrUgm7sE0L7fLZaS%2F4%3D&expires=8h&chkv=0&chkbd=0&chkpc=&dp-logid=6201672027968632425&dp-callid=0&size=c710_u400&quality=100&vuk=-&ft=video)



    A Jupyter Widget


在上面的例子中，很明显$ts1$和$ts2$最相似（它们都是不同变换下的$sin$函数）。 $ts3$显然是最不同的。让我们计算欧几里德距离$d(ts1,ts2)$和$d(ts1,ts3)$，看看欧几里德距离度量是否与我们的直觉告诉我们的一致。让我们首先创建一个函数来计算两个时间序列之间的欧几里德距离。


```python
from math import sqrt
def euclid_dist(t1,t2):
    return sqrt(sum((t1-t2)**2))
```


```python
print('第一组:',euclid_dist(ts1,ts2),'   第二组:',euclid_dist(ts1,ts3))
```

    第一组: 26.959216037969345    第二组: 23.189249190311056
​    

这并不好，因为根据欧几里德距离测量，$ts1$更类似于$ts3$而不是$ts2$，这与我们的直觉相矛盾。这是使用欧几里德距离测量的问题。当它遇到时间轴上时，它经常产生失真问题。解决这个问题的方法是使用动态时间扭曲。

## 动态时间扭曲(Dynamic Time Warping)

动态时间扭曲能在两个时间序列之间找到最佳的非线性对齐。因此，由于时间轴的失真，对准之间的欧几里德距离对于悲观相似性测量更不可接受。然而，要为此付出代价是因为动态时间扭曲在所使用的时间序列的长度上是二次方的。

动态时间扭曲以下列方式工作。考虑两个相同长度$n$的时间序列$Q$和$C$，其中$$Q=q_1,q_2,...,q_n$$和$$C=c_1,c_2,...,c_n$$我们要做的第一件事是构造一个$n\times n$矩阵，其$i,j^{th}$元素是$q_i$和$c_j$之间的欧几里德距离。我们希望找到通过此矩阵的路径，以最小化累积距离。然后，该路径确定两个时间序列之间的最佳对齐。应当注意，时间序列中的一个点可以映射到其他时间序列中的多个点。

让我们调用路径$W$，其中$$W=w_1,w_2,...,w_K$$其中$W$的每个元素表示$Q$中的一个点$i$和$C$中的一个点$j$之间的距离，即$w_k=(q_i-c_j)^2$

因此，我们希望找到具有最小欧几里德距离的路径$$W^*=argmin_W(\sqrt{\sum_{k=1}^Kw_k})$$通过动态编程找到最佳路径，特别是下面的递归函数。 $$\gamma(i,j)=d(q_i,c_j)+min ( \gamma(i-1,j-1),\gamma(i-1,j),\gamma(i,j-1))$$


```python
def DTWDistance(s1, s2):
    DTW={}
    
    for i in range(len(s1)):
        DTW[(i, -1)] = float('inf')
    for i in range(len(s2)):
        DTW[(-1, i)] = float('inf')
    DTW[(-1, -1)] = 0

    for i in range(len(s1)):
        for j in range(len(s2)):
            dist= (s1[i]-s2[j])**2
            DTW[(i, j)] = dist + min(DTW[(i-1, j)],DTW[(i, j-1)], DTW[(i-1, j-1)])
		
    return sqrt(DTW[len(s1)-1, len(s2)-1])
```


```python
print(DTWDistance(ts1,ts2))
print(DTWDistance(ts1,ts3))
```

    17.929718468641138
    21.549494824404572


如您所见，我们的结果已经从我们仅使用欧几里德距离测量时发生了变化。现在，与我们的直觉一致，$ts2$显示出比$ts3$更类似于$ts1$。

### 加快动态时间扭曲

动态时间扭曲具有$O(nm)$的复杂性，其中$n$是第一个时间序列的长度，$m$是第二个时间序列的长度。如果您在长时间序列数据上多次执行动态时间扭曲，这可能会非常昂贵。但是，有几种方法可以加快速度。第一种是强制执行局部性约束。这是假设如果$i$和$j$相距太远则不能匹配$q_i$和$c_j$。阈值由窗口大小$w$确定。这样，仅考虑该窗口内的映射，这加速了内循环。以下是修改后的代码，其中包括窗口大小$w$。


```python
def DTWDistance(s1, s2,w):
    DTW={}
    
    w = max(w, abs(len(s1)-len(s2)))
    
    for i in range(-1,len(s1)):
        for j in range(-1,len(s2)):
            DTW[(i, j)] = float('inf')
    DTW[(-1, -1)] = 0
  
    for i in range(len(s1)):
        for j in range(max(0, i-w), min(len(s2), i+w)):
            dist= (s1[i]-s2[j])**2
            DTW[(i, j)] = dist + min(DTW[(i-1, j)],DTW[(i, j-1)], DTW[(i-1, j-1)])
		
    return sqrt(DTW[len(s1)-1, len(s2)-1])
```


```python
print(DTWDistance(ts1,ts2,10))
print(DTWDistance(ts1,ts3,10))
```

    18.59655183841726
    22.47248284679103

另一种加快速度的方法是使用动态时间扭曲的_LB Keogh_下限。它被定义为$$LBKeogh(Q,C)=\sum_{i=1}^n (c_i-U_i)^2I(c_i > U_i)+(c_i-L_i)^2I(c_i < L_i)$$

其中$$U_i​$$和$$L_i​$$是时间序列$Q​$的上限和下限，定义为$$ U_i=max(q_{i-r}:q_{i+r})​$$，$$ L_i=min(q_{i-r}:q_{i+r})​$$定义为达到$r​$，$$I(\cdot) ​$$是指标函数。它可以使用以下功能实现。


```python
def LB_Keogh(s1,s2,r):
    LB_sum=0
    for ind,i in enumerate(s1):
        
        lower_bound=min(s2[(ind-r if ind-r>=0 else 0):(ind+r)])
        upper_bound=max(s2[(ind-r if ind-r>=0 else 0):(ind+r)])
        
        if i>upper_bound:
            LB_sum=LB_sum+(i-upper_bound)**2
        elif i<lower_bound:
            LB_sum=LB_sum+(i-lower_bound)**2
    
    return sqrt(LB_sum)
```

让我们来看看现在ts1与另外两个序列的距离


```python
print(LB_Keogh(ts1,ts2,20))
print(LB_Keogh(ts1,ts3,20))
```

    6.253892351594148
    19.959547869419758


_LB Keogh_下界方法是线性的，而动态时间扭曲是复杂性的二次方，这使得它在搜索大量时间序列时非常有利。

## 分类以及聚类
现在我们有一个可靠的方法来确定两个时间序列之间的相似性，我们可以使用k-NN算法进行分类。根据经验，当$k=1$时，效果最好。以下是使用动态时间扭曲欧几里德距离的1-NN算法。在该算法中，$train$是时间序列示例的训练集，其中时间序列所属的类被附加到时间序列的末尾。 $test$是您试图预测其对应类的测试集。在该算法中，对于测试集中的每个时间序列，必须通过训练集中的所有点执行搜索，以便找到最相似的点。鉴于动态时间扭曲的时间复杂度是二次的，这计算时间显得过于奢侈。我们可以使用_LB Keogh_下限来加速分类。计算_LB Keogh_比执行动态时间扭曲要“便宜”得多。从$LB Keogh(Q,C) \leq DTW(Q,C)$开始，我们可以消除与当前最相似的时间序列以外的不可能更相似的时间序列。通过这种方式，我们消除了许多不必要的动态时间扭曲计算时间。


```python
from sklearn.metrics import classification_report

def knn(train,test,w):
    preds=[]
    for ind,i in enumerate(test):
        min_dist=float('inf')
        closest_seq=[]
        #print ind
        for j in train:
            if LB_Keogh(i[:-1],j[:-1],5)<min_dist:
                dist=DTWDistance(i[:-1],j[:-1],w)
                if dist<min_dist:
                    min_dist=dist
                    closest_seq=j
        preds.append(closest_seq[-1])
    return classification_report(test[:,-1],preds)
```

现在让我们对一些数据进行测试。我们将使用4的窗口大小。尽管使用了_LB Keogh_绑定和动态时间扭曲局部性约束来加速代码，但我们仍可能需要几分钟才能运行，下面先来加载数据，并可视化视图。


```python
train = np.genfromtxt('datasets/train.csv', delimiter='\t')
test = np.genfromtxt('datasets/test.csv', delimiter='\t')
```


```python
train = pd.DataFrame(train)
test = pd.DataFrame(test)
tem=qgrid.show_grid(test,show_toolbar=True)
tem
##train = np.genfromtxt('datasets/train.csv', delimiter='\t')
##test = np.genfromtxt('datasets/test.csv', delimiter='\t')
```


    A Jupyter Widget



```python
print(knn(train,test,4))
```

                 precision    recall  f1-score   support

            1.0       1.00      0.96      0.98        50
            2.0       0.96      1.00      0.98        50
            3.0       1.00      1.00      1.00        50
            4.0       0.98      1.00      0.99        50
            5.0       1.00      1.00      1.00        50
            6.0       1.00      0.98      0.99        50
    
    avg / total       0.99      0.99      0.99       300

​    

同样的想法也可以应用于k均值聚类。在该算法中，簇的数量设置为_apriori_，并且类似的时间序列被聚集在一起。下面是实现的代码


```python
import random

def k_means_clust(data,num_clust,num_iter,w=5):
    centroids=random.sample(data,num_clust)
    counter=0
    for n in range(num_iter):
        counter+=1
        print (counter)
        assignments={}
        #assign data points to clusters
        for ind,i in enumerate(data):
            min_dist=float('inf')
            closest_clust=None
            for c_ind,j in enumerate(centroids):
                if LB_Keogh(i,j,5)<min_dist:
                    cur_dist=DTWDistance(i,j,w)
                    if cur_dist<min_dist:
                        min_dist=cur_dist
                        closest_clust=c_ind
            if closest_clust in assignments:
                assignments[closest_clust].append(ind)
            else:
                assignments[closest_clust]=[]
    
        #recalculate centroids of clusters
        for key in assignments:
            clust_sum=0
            for k in assignments[key]:
                clust_sum=clust_sum+data[k]
            centroids[key]=[m/len(assignments[key]) for m in clust_sum]
    
    return centroids
        
```

